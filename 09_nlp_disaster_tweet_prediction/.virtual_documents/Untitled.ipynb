import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.naive_bayes import MultinomialNB


df_train = pd.read_csv("train.csv")
df_test = pd.read_csv("test.csv")


df_train.head()


y_train = df_train["target"]


data_train = df_train["text"].to_numpy()
data_test = df_test["text"].to_numpy()


data_train


tokenizer = RegexpTokenizer("\w+")
sw = set(stopwords.words("english"))
ps = PorterStemmer()


def cleanSentence(sent):
    sent = sent.lower()
    words = tokenizer.tokenize(sent)
    cleaned_sent_arr = [w for w in words if w not in sw]
    stemmed_words = [ps.stem(token) for token in cleaned_sent_arr]
    cleaned_sent = " ".join(stemmed_words)
    return cleaned_sent


def getDoc(document):
    d = []
    for doc in document:
        d.append(cleanSentence(doc))
    return np.array(d)


cleaned_data_train = getDoc(data_train)
cleaned_data_test = getDoc(data_test)


data_train


cleaned_data_train


cv = CountVectorizer()


X_train = cv.fit_transform(cleaned_data_train).toarray()
X_test = cv.transform(cleaned_data_test).toarray()


model = MultinomialNB()


model.fit(X_train, y_train)


X_train.shape, y_train.shape


ans = model.predict(X_test)


df_ans = pd.DataFrame({"id": df_test["id"], "target": ans})


df_ans.to_csv("outputs/ans5.csv", index=None)



