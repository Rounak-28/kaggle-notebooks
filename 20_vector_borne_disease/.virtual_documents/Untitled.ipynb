import torch
import torch.nn as nn

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


train_val = pd.read_csv("data/train.csv")
test = pd.read_csv("data/test.csv")


train_val.head(2)


test.head(2)


X_train_val = train_val.drop(["id", "prognosis"], axis=1).to_numpy()
X_test = test.drop(["id"], axis=1).to_numpy()


cv = CountVectorizer()
y_train_val = cv.fit_transform(train_val.prognosis.to_numpy()).toarray()


X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)


# X_train = torch.tensor(X_train).float()
# X_val = torch.tensor(X_val).float()
# y_train = torch.tensor(y_train).float()
# y_val = torch.tensor(y_val).float()

# X_test = torch.tensor(X_test).float()


print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape)


# model = XGBClassifier()
model = LogisticRegression(max_iter=500)


model.fit(X_train, np.argmax(y_train, 1))


pred_test = model.predict_proba(X_test)


# in_features = X_train.shape[1]
# hidden_features = 10
# out_features = y_train.shape[1]


# device = "cpu"


# class NeuralNetwork(nn.Module):
#     def __init__(self, in_features, hidden_features, out_features):
#         super().__init__()
#         self.layer = nn.Sequential(
#             nn.Linear(in_features, hidden_features),
#             nn.ReLU(),
#             nn.Linear(hidden_features, hidden_features),
#             nn.ReLU(),
#             nn.Linear(hidden_features, out_features),
#             nn.Tanh()
#         )

#     def forward(self, x):
#         x = self.layer(x)
#         return x

# model = NeuralNetwork(in_features, hidden_features, out_features).to(device)
# print(model)


# loss_fn = nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)


# epochs = 10
# for i in range(epochs):
#     pred = model(X_train)
#     loss = loss_fn(pred, y_train)

#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()

#     print(f"epoch: {i}, loss: {loss.item():.4f}")


# pred_val = torch.argmax(model(X_val), 1)

# accuracy_score(torch.argmax(y_val, 1), pred_val)


# pred_test = model(X_test)


pred_test.shape


pred_ans = torch.topk(torch.tensor(pred_test), 3)[1].numpy()


# pred_ans


ans = [_.tolist() for _ in cv.inverse_transform(pred_ans)]


for idk in ans:
    for i in range(len(idk)):
        idk[i] = idk[i].capitalize()


# ans


final = [" ".join(_) for _ in ans]


len(final)


ans_df = pd.DataFrame({"id": test.id, "prognosis": final})


ans_df.head()


ans_df.to_csv("outputs/ans3.csv", index=False)



